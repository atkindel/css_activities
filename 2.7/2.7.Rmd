---
title: "2.7: Google Culture?"
author: "Alex Kindel"
date: "November 8, 2016"
output: html_document
---

```{r setup, include=FALSE}
require(dplyr)
require(knitr)
require(ggplot2)
require(sqldf)
require(magrittr)

opts_chunk$set(fig.align="center")
```

### Works referenced

Michel, Jean-Baptiste, Yuan Kui Shen, Aviva P. Aiden, Adrian Veres, Matthew K. Gray, The Google Books Team, Joseph P. Pickett, et al. 2011. “Quantitative Analysis of Culture Using Millions of Digitized Books.” Science 331 (6014): 176–82. doi:10.1126/science.1199644.

Pechenick, Eitan Adam, Christopher M. Danforth, and Peter Sheridan Dodds. 2015. “Characterizing the Google Books Corpus: Strong Limits to Inferences of Socio-Cultural and Linguistic Evolution.” PLoS ONE 10 (10): e0137041. doi:10.1371/journal.pone.0137041.

Google Books Ngram Viewer. http://storage.googleapis.com/books/ngrams/books/datasetsv2.html.

### Background

Michel et al. (2011) constructed a corpus emerging from Google’s effort to digitize books. Using the first version of the corpus, which was published in 2009 and contained over 5 million digitized books, the authors analyzed word usage frequency to investigate linguistic changes and cultural trends. Soon the Google Books Corpus became a popular data source for researchers, and a 2nd version of the database was released in 2012.

However, Pechenick, Danforth, and Dodds (2015) warned that researchers need to fully characterize the sampling process of the corpus before using it for drawing broad conclusions. The main issue is that the corpus is library-like, containing one of each book. As a result, an individual, prolific author is able to noticeably insert new phrases into the Google Books lexicon. Moreover, scientific texts constitute an increasingly substantive portion of the corpus throughout the 1900s. In addition, by comparing two versions of the English Fiction datasets, Pechenick et al. found evidence that insufficient filtering was used in producing the first version.

Using the Google NGram data, Michel et al. argue that we are forgetting faster and faster. In Fig. 3a, they plot the usage trajectories for three years: 1883, 1910, and 1950. Then, they calculate the “half-life” of each year for all years between 1875 and 1975. They argue that we are forgetting the past faster and faster (see inset of Fig 3a). This calculation was done with Version 1 of the English corpus. In this activity we are going to replicate and extend this result using Version 2 of the corpus.

```{r get_data, echo=FALSE, eval=FALSE}
setwd("~/Code/css_activities/2.7")
system("./preprocess_ng2.sh")
```

```{r preprocess, echo=FALSE, eval=FALSE} 
system("./find_years.sh")
```

### A

First, we retrieve the data from Google Books, pre-process it to extract our years of interest, and load it into R:

```{r 2.7a}
# Ngrams data
ng_years <- read.csv("data/ngyears.tsv", sep="\t", colClasses = c("character", rep("integer", 3)))

# Add total counts and clean up data
total_count <- read.csv("data/total_counts.csv")
ng_years <- sqldf("SELECT n.ngram, n.year, n.match_count, n.volume_count,
                  t.match_count as match_total, t.volume_count as volume_total
                  FROM ng_years n
                  LEFT JOIN total_count t USING(year)
                  WHERE NOT n.ngram LIKE '%.%'")
```

### B

We plot the frequencies of these terms in our sample of published books as a function of publication year:

```{r 2.7b}
ng_years %>% 
  filter(year > 1875) %>%
  filter(ngram %in% c('1883', '1910', '1950')) %>%
  group_by(ngram) %>%
  ggplot(aes(x=year, y=match_count, color=ngram)) + 
  geom_line() + 
  scale_color_manual(values=c("blue", "green", "red"))
```

At first glance these data seem like they might tell a different story from the earlier wave of Ngrams data. The trend no longer appears to be monotonic!

### C

We can check this against Google's Ngram viewer:

<center><iframe height="500" width="900" src="https://books.google.com/ngrams/graph?content=1883%2C1910%2C1950&year_start=1850&year_end=2008&corpus=15&smoothing=3&share=&direct_url=t1%3B%2C1883%3B%2Cc0%3B.t1%3B%2C1910%3B%2Cc0%3B.t1%3B%2C1950%3B%2Cc0" style="border: 0;"></iframe></center>

Their results seem to match the Michel et al. figure.

### D

To see why, let's try normalizing our year frequencies by the total number of terms in a given year:

```{r 2.7d}
ng_years %>% 
  mutate(match_prop = match_count/match_total) %>%
  filter(year > 1850) %>%
  filter(ngram %in% c('1883', '1910', '1950')) %>%
  group_by(ngram) %>%
  ggplot(aes(x=year, y=match_prop, color=ngram)) + 
  geom_line() + 
  scale_color_manual(values=c("blue", "green", "red"))
```

When we do this, we see that the recent increase in raw counts is actually a reduction in the *proportion* of matches. So, we recover the Michel et al./Google Ngram Viewer result. This suggests the Michel et al. paper was using net match proportions, not gross match frequencies.

### E.

The result seems more or less solid.

### F.

Next, we're interested in calculating the "half-life" of a given year. We calculate this value by finding the date in which the proportion of a year's mentions in a given year first falls below half the maximum historical proportion of mentions we observe. The half-life is simply the number of years that have elapsed since the year where the maximum proportion was observed.

```{r 2.7e_i}
cutoff <- 1800
# Calculate half life for data on a given year
half_life <- function(v) {
  # Get year of maximum proportion observed
  v %<>% mutate(match_prop = match_count/match_total) %>% filter(year > cutoff)
  year.max <- v[which.max(v$match_prop),'year']  # Year when maximum proportion observed
  
  # Calculate proportionate half life
  prop.max <- v %>% filter(year == year.max) %>% select(match_prop)  # Max proportion
  hl.prop <- (prop.max / 2)[1,1]
  
  # Determine half life in years
  v %>%
    filter(year >= year.max) %>%
    filter(match_prop <= hl.prop) %>%
    select(year) %>%
    slice(1) ->
    half.year
  return(half.year - year.max)
}

# Calculate half life for given ngram
hl.apply <- function(ngr) {
  ng_years %>%
    filter(ngram == ngr) %>%
    half_life() -> hl
  return(hl[1,1])
}

# Get year ngrams
total_count %>%
  filter(year > cutoff) %>%
  select(year) ->
  yr
years <- as.character(yr$year)

# Calculate half life for all ngrams
sapply(years, hl.apply) -> hls
half.lives <- data.frame(year = names(hls), half.life = hls)

# Plot and examine trend
half.lives %>%
  filter(!!half.life) %>%
  ggplot(aes(x=strtoi(year), y=half.life)) + 
  geom_point() +
  geom_smooth(method="loess") +
  geom_point(data=filter(half.lives, year == '1883'), color="blue", size=4, alpha=0.6) + 
  geom_point(data=filter(half.lives, year == '1910'), color="green", size=4, alpha=0.6) + 
  geom_point(data=filter(half.lives, year == '1950'), color="red", size=4, alpha=0.6)
```

Note that this doesn't look anything like their half-life plot. This could be an Ngrams 2.0 phenomenon, or it could be something else. I'm not sure I buy that inset to begin with. Our data is very noisy in the first half of the 19th century. Substantial outliers (half-life > 80 years) include 1812 (not surprising, since it has a whole war named after it!), 1820 (the Missouri Compromise...?), 1824 (presidential election decided by the House of Representatives for the first and only time in history), and 1830 (publication of the Book of Mormon). Compare this to much more forgettable years like 1776 (HL = 3 years), 1914 (HL = 13 years), or 1945 (HL = 11 years) and a story begins to emerge about why collective memory may be more complicated than these counts initially suggested.

```{r 2.7e_ii}
ng_years %>% 
  filter(year > 1800) %>%
  filter(ngram %in% c('1812', '1820', '1824', '1830', '1883')) %>%
  group_by(ngram) %>%
  ggplot(aes(x=year, y=match_count/match_total, color=ngram)) + 
  geom_line() + 
  scale_color_manual(values=c("blue", "green", "red", "purple", "orange"))
```

```{r 2.7e_iii}
ng_years %>% 
  filter(year > 1700) %>%
  filter(ngram %in% c('1776', '1914', '1945', '1950')) %>%
  group_by(ngram) %>%
  ggplot(aes(x=year, y=match_count/match_total, color=ngram)) + 
  geom_line() + 
  scale_color_manual(values=c("blue", "green", "red", "purple", "orange"))
```

## G.

Let's look at these counts and "half-lives" by volume, rather than by total matches:

```{r 2.7g_i}
ng_years %>% 
  mutate(volume_prop = volume_count/volume_total) %>%
  filter(year > 1850) %>%
  filter(ngram %in% c('1883', '1910', '1950')) %>%
  group_by(ngram) %>%
  ggplot(aes(x=year, y=volume_prop, color=ngram)) + 
  geom_line() + 
  scale_color_manual(values=c("blue", "green", "red"))
```

Our "decay" plots for the three look somewhat similar, but there are some critical differences. First, observe that the plots are more comparable in scale. Second, notice the discrete spike for the year of interest. The decline in the subsequent year is not nearly as dramatic in more recent years. Also note that the tail of older years flattens out, whereas the tail for more recent years becomes steeper.

```{r 2.7g_ii}
cutoff <- 1800
# Calculate half life for data on a given year
half_life <- function(v) {
  # Get year of maximum proportion observed
  v %<>% mutate(volume_prop = volume_count/volume_total) %>% filter(year > cutoff)
  year.max <- v[which.max(v$volume_prop),'year']  # Year when maximum proportion observed
  
  # Calculate proportionate half life
  prop.max <- v %>% filter(year == year.max) %>% select(volume_prop)  # Max proportion
  hl.prop <- (prop.max / 2)[1,1]
  
  # Determine half life in years
  v %>%
    filter(year >= year.max) %>%
    filter(volume_prop <= hl.prop) %>%
    select(year) %>%
    slice(1) ->
    half.year
  return(half.year - year.max)
}

# Calculate half life for given ngram
hl.apply <- function(ngr) {
  ng_years %>%
    filter(ngram == ngr) %>%
    half_life() -> hl
  return(hl[1,1])
}

# Get year ngrams
total_count %>%
  filter(year > cutoff) %>%
  select(year) ->
  yr
years <- as.character(yr$year)

# Calculate half life for all ngrams
sapply(years, hl.apply) -> hls
half.lives <- data.frame(year = names(hls), half.life = hls)

# Plot and examine trend
half.lives %>%
  filter(!!half.life) %>%
  ggplot(aes(x=strtoi(year), y=half.life)) + 
  geom_point() +
  geom_smooth(method="loess") +
  geom_point(data=filter(half.lives, year == '1883'), color="blue", size=4, alpha=0.6) + 
  geom_point(data=filter(half.lives, year == '1910'), color="green", size=4, alpha=0.6) + 
  geom_point(data=filter(half.lives, year == '1950'), color="red", size=4, alpha=0.6)
```

When we plot our half lives, the story becomes a little bit more complicated than the original "we forget faster and faster!" explanation. Based on volume mentions, it looks like the half-life of years actually increased during the late 19th century and the first half of the 20th century, with the notable exception of a major dip during World War II (1939-45). Volume-based year mention half-lives decrease rapidly after 1950.